{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler, OneHotEncoder\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation,BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import models\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD,Adagrad\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping,TensorBoard\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatest = pd.read_csv(\"gender_age_test.csv\",index_col='device_id')\n",
    "phone=pd.read_csv(\"phone_brand_device_model.csv\")\n",
    "app_label=pd.read_csv('app_labels.csv')\n",
    "label_cat=pd.read_csv(\"label_categories.csv\")\n",
    "app_events=pd.read_csv(\"app_events.csv\", dtype={'is_active':bool})\n",
    "events = pd.read_csv('events.csv',  parse_dates=['timestamp'],index_col='event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicate device id's\n",
    "phone = phone.drop_duplicates('device_id',keep='first').set_index('device_id') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112071, 0)\n",
      "(186716, 2)\n",
      "(459943, 2)\n",
      "(930, 2)\n",
      "(32473067, 4)\n",
      "(3252950, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(gatest.shape)\n",
    "print(phone.shape)\n",
    "print(app_label.shape)\n",
    "print(label_cat.shape)\n",
    "print(app_events.shape)\n",
    "print(events.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING PICKLE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('brandencoder','rb') as fp:\n",
    "    brand_encoder = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('modelencoder','rb') as fp:\n",
    "    model_encoder = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('appencoder','rb') as fp:\n",
    "    app_encoder = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('labelencoder','rb') as fp:\n",
    "    label_encoder = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hour_tfidf','rb') as fp:\n",
    "    hour_tfidf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hour_bow','rb') as fp:\n",
    "    hour_bow= pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hour_bin_bow','rb') as fp:\n",
    "    hour_bin_bow = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('day_tfidf','rb') as fp:\n",
    "    day_tfidf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lat_scaler','rb') as fp:\n",
    "    lat_scaler = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lon_scaler','rb') as fp:\n",
    "    lon_scaler = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('clustered_features','rb') as fp:\n",
    "    clustered_features = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('isactive_tfidf','rb') as fp:\n",
    "    isactive_tfidf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('class_columns','rb') as fp:\n",
    "    classes = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model_onehot','rb') as fp:\n",
    "    model_onehot = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('brand_onehot','rb') as fp:\n",
    "    brand_onehot = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('kmeans_labels','rb') as fp:\n",
    "    kmeans_labels = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_function_3(gatest,phone,app_label,label_cat,app_events,events):\n",
    "    start=datetime.now()\n",
    "    mask=np.in1d(gatest.index,events[\"device_id\"].values)\n",
    "    gatest_events= gatest[mask]\n",
    "    mask=np.in1d(gatest.index,events[\"device_id\"].values,invert=True)\n",
    "    gatest_noevents= gatest[mask]\n",
    "    if(gatest_noevents.shape[0] ==1):\n",
    "        gatest['testrow'] = np.arange(gatest.shape[0])\n",
    "        gatest_noevents['testrow']=np.arange(gatest_noevents.shape[0])\n",
    "        \n",
    "        gatest_noevents['model']=phone['device_model']\n",
    "        gatest_noevents['brand']=phone['phone_brand']\n",
    "        \n",
    "        gatest_noevents['model'] = str(phone['device_model'])\n",
    "        gatest_noevents['brand'] = str(phone['phone_brand'])\n",
    "        gatest_noevents_model = model_onehot.transform(gatest_noevents['model'])\n",
    "        gatest_noevents_brand= brand_onehot.transform(gatest_noevents['brand'])\n",
    "        xtest_noevents=hstack((gatest_noevents_brand, gatest_noevents_model), format='csr')\n",
    "        model_list_1=[]\n",
    "        for i in range(5):\n",
    "            model=load_model('nn_onehot '+str(i+1))\n",
    "            model_list_1.append(model)\n",
    "            avg_pred1=np.zeros((xtest_noevents.shape[0],12))\n",
    "        for i in range(len(model_list_1)):\n",
    "            test_pred=model_list_1[i].predict_proba(xtest_noevents)\n",
    "            avg_pred1+=test_pred\n",
    "        avg_pred1/=len(model_list_1)\n",
    "        print(classes)\n",
    "        print(avg_pred1)\n",
    "\n",
    "    if (gatest_events.shape[0] == 1):\n",
    "        gatest['testrow'] = np.arange(gatest.shape[0])\n",
    "        gatest_events['testrow']=np.arange(gatest_events.shape[0])\n",
    "\n",
    "        phone['brand'] = brand_encoder.transform(phone['phone_brand'])\n",
    "        nbrand=len(brand_encoder.classes_)\n",
    "        m=phone['phone_brand'].str.cat(phone['device_model'])\n",
    "        phone['model'] = model_encoder.transform(m)\n",
    "        nmodel=len(model_encoder.classes_)\n",
    "    \n",
    "        app_events['app'] = app_encoder.transform(app_events['app_id'])\n",
    "        napps = len(app_encoder.classes_)\n",
    "        deviceapps = (app_events.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n",
    "                               .groupby(['device_id','app'])['app'].agg(['size'])# grouping by device id and app and finding size of app\n",
    "                               .merge(gatest_events[['testrow']], how='left', left_index=True, right_index=True)#finding testrow\n",
    "                               .reset_index())\n",
    "        app_label = app_label.loc[app_label.app_id.isin(app_events.app_id.unique())]\n",
    "        app_label['app'] = app_encoder.transform(app_label.app_id)\n",
    "        app_label['label'] = label_encoder.transform(app_label.label_id)\n",
    "        nlabels = len(label_encoder.classes_)\n",
    "        devicelabels = (deviceapps[['device_id','app']]\n",
    "                        .merge(app_label[['app','label']])\n",
    "                        .groupby(['device_id','label'])['app'].agg(['size'])\n",
    "                        .merge(gatest_events[['testrow']], how='left', left_index=True, right_index=True)\n",
    "                        .reset_index())\n",
    "        events['hour'] = events['timestamp'].map(lambda x:pd.to_datetime(x).hour)\n",
    "        events['hourbin'] = [1 if ((x>=1)&(x<=6)) else 2 if ((x>=7)&(x<=12)) else 3 if ((x>=13)&(x<=18)) else 4 for x in events['hour']]\n",
    "        events.hour=events.hour.astype(str)\n",
    "        events.hourbin=events.hourbin.astype(str)\n",
    "        hourjoin = events.groupby(\"device_id\")[\"hour\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "        hourbinjoin=events.groupby(\"device_id\")[\"hourbin\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "        daysjoin=events['timestamp'].dt.day_name()\n",
    "        events['day']=daysjoin.map({'Sunday':0,'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6})\n",
    "        daysjoin = events.groupby(\"device_id\")[\"day\"].apply(lambda x: \" \".join(\"0\"+str(s) for s in x))\n",
    "        median_lat = events.groupby(\"device_id\")[\"latitude\"].agg('median')\n",
    "        median_lon=events.groupby(\"device_id\")[\"longitude\"].agg('median')\n",
    "        com=pd.concat([median_lat, median_lon], axis=1)\n",
    "        clustered_geo_features=pd.Series(kmeans_labels)\n",
    "        clustered_geo_features.index=median_lon.index\n",
    "        apps = app_events.groupby(\"event_id\")[\"is_active\"].apply(lambda x: \" \".join(str(s) for s in x))\n",
    "        events[\"apps_active\"] = events.index.map(apps)\n",
    "        active_apps_events = events.groupby(\"device_id\")[\"apps_active\"].apply(lambda x: \" \".join(str(s) for s in x if str(s)!='nan'))\n",
    "        gatest_events['brand']=phone['brand']\n",
    "        Xte_events_brand = csr_matrix((np.ones(gatest_events.shape[0]), # Number of Rows/Devices\n",
    "                               (gatest_events.testrow, gatest_events.brand)),shape=(gatest_events.shape[0],nbrand))\n",
    "        gatest_events['model']=phone['model']\n",
    "        Xte_events_model = csr_matrix((np.ones(gatest_events.shape[0]), \n",
    "                               (gatest_events.testrow, gatest_events.model)),shape=(gatest_events.shape[0],nmodel))\n",
    "        d = deviceapps.dropna(subset=['testrow'])\n",
    "        Xte_events_app = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.app)), \n",
    "                              shape=(gatest_events.shape[0],napps))\n",
    "        d = devicelabels.dropna(subset=['testrow'])\n",
    "        Xte_events_labels = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), \n",
    "                              shape=(gatest_events.shape[0],nlabels))\n",
    "        gatest_events[\"hourjoin\"]=gatest_events.index.map(hourjoin)\n",
    "        X_te_hourjoin_tfidf = hour_tfidf.transform(gatest_events['hourjoin'].values)\n",
    "        gatest_events[\"hourbinjoin\"]=gatest_events.index.map(hourbinjoin)\n",
    "        X_te_hourbinjoin_onehot = hour_bin_bow.transform(gatest_events['hourbinjoin'].values)\n",
    "        gatest_events[\"daysjoin\"]=gatest_events.index.map(daysjoin)\n",
    "        X_te_daysjoin_tfidf = day_tfidf.transform(gatest_events['daysjoin'].values)\n",
    "        gatest_events[\"latitude\"]=gatest_events.index.map(median_lat)\n",
    "        X_te_event_lat = lat_scaler.transform(gatest_events['latitude'].values.reshape(-1,1))\n",
    "        gatest_events[\"longitude\"]=gatest_events.index.map(median_lon)\n",
    "        X_te_event_lon = lon_scaler.transform(gatest_events['longitude'].values.reshape(-1,1))\n",
    "        gatest_events[\"locationbin\"]=gatest_events.index.map(clustered_geo_features)\n",
    "        X_te_clus = clustered_features.transform(gatest_events['locationbin'].values.reshape(-1,1))\n",
    "        gatest_events['apps_active']=gatest_events.index.map(active_apps_events)\n",
    "        X_te_active = isactive_tfidf.transform(gatest_events['apps_active'].values)\n",
    "    \n",
    "        X_test_events =hstack((Xte_events_brand,Xte_events_model,Xte_events_labels,X_te_hourjoin_tfidf,X_te_hourbinjoin_onehot,X_te_daysjoin_tfidf,X_te_event_lat,X_te_event_lon,Xte_events_app,X_te_active,X_te_clus),format='csr')\n",
    "\n",
    "        model_list_1=[]\n",
    "        for i in range(5):\n",
    "            model=load_model('nn1'+str(i+1))\n",
    "            model_list_1.append(model)\n",
    "        avg_pred2=np.zeros((X_test_events.shape[0],12))\n",
    "        for i in range(len(model_list_1)):\n",
    "            test_pred=model_list_1[i].predict_proba(X_test_events)\n",
    "            avg_pred2+=test_pred\n",
    "        avg_pred2/=len(model_list_1)\n",
    "        model_list_1=[]\n",
    "        for i in range(5):\n",
    "            model=load_model('nn2'+str(i+1))\n",
    "            model_list_1.append(model)\n",
    "        avg_pred3=np.zeros((X_test_events.shape[0],12))\n",
    "        for i in range(len(model_list_1)):\n",
    "            test_pred=model_list_1[i].predict_proba(X_test_events)\n",
    "            avg_pred3+=test_pred\n",
    "        avg_pred3/=len(model_list_1)\n",
    "        test2=(0.5*avg_pred2)+(0.5*avg_pred3)\n",
    "        print(classes)\n",
    "        print(test2)\n",
    "        \n",
    "    if(gatest_events.shape[0] == gatest_noevents.shape[0]):\n",
    "        print('device id is not present')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F23-' 'F24-26' 'F27-28' 'F29-32' 'F33-42' 'F43+' 'M22-' 'M23-26'\n",
      " 'M27-28' 'M29-31' 'M32-38' 'M39+']\n",
      "[[2.0000000e-01 0.0000000e+00 0.0000000e+00 2.0000000e-01 0.0000000e+00\n",
      "  9.8520795e-17 2.0000000e-01 2.0000000e-01 0.0000000e+00 0.0000000e+00\n",
      "  2.0000000e-01 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "final_function_3(gatest[4:5],phone,app_label,label_cat,app_events,events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F23-' 'F24-26' 'F27-28' 'F29-32' 'F33-42' 'F43+' 'M22-' 'M23-26'\n",
      " 'M27-28' 'M29-31' 'M32-38' 'M39+']\n",
      "[[3.92903631e-04 1.07207675e-03 2.00781806e-03 7.64651578e-03\n",
      "  3.69362756e-02 5.11963476e-02 3.27780007e-03 2.34491333e-02\n",
      "  2.80932087e-02 9.01987899e-02 2.58072273e-01 4.97656864e-01]]\n"
     ]
    }
   ],
   "source": [
    "final_function_3(gatest[0:1],phone,app_label,label_cat,app_events,events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
